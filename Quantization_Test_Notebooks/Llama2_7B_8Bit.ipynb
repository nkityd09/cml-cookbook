{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6509d06-3cde-4026-ac42-b76a21962d4a",
   "metadata": {},
   "source": [
    "# Llama-2 7B 8-Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf0a40-7ab5-467a-baeb-1b420c62c262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:50<00:00, 25.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:8091\n",
      "Running on public URL: https://d19b66229e35836c2d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d19b66229e35836c2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio app ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import textwrap\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "### Multi-document retriever\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, VectorDBQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import glob\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import uuid\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.prompts import PromptTemplate\n",
    "langchain.verbose = True\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "IP_ADDR=\"18.188.2.246\"\n",
    "chroma = chromadb.HttpClient(host=IP_ADDR, port=8000)\n",
    "\n",
    "access_token = \"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\"\n",
    "hugging_face_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hugging_face_model, use_auth_token=access_token)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(hugging_face_model, #meta-llama/Llama-2-13b-chat-hf\n",
    "                                                     load_in_8bit=True,\n",
    "                                                     device_map='auto',\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     low_cpu_mem_usage=True,\n",
    "                                                     use_auth_token=access_token\n",
    "                                                    )\n",
    "max_len = 8192\n",
    "llm_task = \"text-generation\"\n",
    "T = 0.1\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    task=llm_task,\n",
    "    model=llm_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_len,\n",
    "    temperature=T,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "#Uploading Files to target location\n",
    "target = '/home/cdsw/data/'\n",
    "def upload_file(files):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    file_paths = [file.name for file in files]\n",
    "    print(file_paths)\n",
    "    for file in file_paths:\n",
    "        shutil.copy(file, target)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "#Embedding function which will be used to convert the text to Vector Embeddings\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") #TODO: Find replacement\n",
    "\n",
    "#Defining LangChain's Chroma Client \n",
    "langchain_chroma = Chroma(\n",
    "client=chroma,\n",
    "collection_name=\"default\",\n",
    "embedding_function=embedding_function)\n",
    "\n",
    "#retriever = langchain_chroma.as_retriever(search_kwargs={\"k\": 3, \"search_type\" : \"similarity\"})\n",
    "\n",
    "def create_default_collection():\n",
    "    \"\"\"\n",
    "    Create Default Collection in ChromaDB if no collections exists\n",
    "    \"\"\"\n",
    "    chromadb.create_collection(\"default\")\n",
    "    return \"Created Default Collection\"\n",
    "\n",
    "\n",
    "\n",
    "def collection_lists():\n",
    "    \"\"\"\n",
    "    List All Collections available in ChromaDB\n",
    "    \"\"\"\n",
    "    collection_list = []\n",
    "    chroma_collections = chroma.list_collections()\n",
    "    if chroma_collections == None:\n",
    "        create_default_collection()\n",
    "    else:\n",
    "        for collection in chroma_collections:\n",
    "            collection_list.append(collection.name)\n",
    "    return collection_list\n",
    "\n",
    "collection_list = collection_lists()\n",
    "\n",
    "def embed_documents(collection):\n",
    "    \"\"\"\n",
    "    Given a collection name, this function loads PDF documents from a specified directory, preprocesses their content, \n",
    "    and then embeds the documents into a vector database using Chroma. \n",
    "    After embedding, it deletes the processed PDFs from the directory.\n",
    "    \n",
    "    Args:\n",
    "    - collection (str): Name of the collection to be used in the Chroma vector database.\n",
    "\n",
    "    Returns:\n",
    "    - output (str): Message indicating which documents have been embedded.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\"/home/cdsw/data/\",\n",
    "                         glob=\"**/*.pdf\",\n",
    "                         loader_cls=PyPDFLoader,\n",
    "                         use_multithreading=True)\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    for i in range(len(documents)):\n",
    "        documents[i].page_content = documents[i].page_content.replace('\\t', ' ')\\\n",
    "                                                         .replace('\\n', ' ')\\\n",
    "                                                         .replace('       ', ' ')\\\n",
    "                                                         .replace('      ', ' ')\\\n",
    "                                                         .replace('     ', ' ')\\\n",
    "                                                         .replace('    ', ' ')\\\n",
    "                                                         .replace('   ', ' ')\\\n",
    "                                                         .replace('  ', ' ')\n",
    "\n",
    "\n",
    "    langchain_chroma = Chroma(\n",
    "    client=chroma,\n",
    "    collection_name=collection,\n",
    "    embedding_function=embedding_function)    \n",
    "\n",
    "    collection = chroma.get_collection(collection) # Needs to be initialized as LangChain cannot add texts\n",
    "\n",
    "    # Document is chunked per page. Each page will be an entry in the Vector DB\n",
    "    for doc in documents: \n",
    "        collection.add(\n",
    "            ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n",
    "        )\n",
    "\n",
    "    pattern = \"/home/cdsw/data/*.pdf\"\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    output = f\"Documents have been embedded: {files}\"\n",
    "    print(output)\n",
    "    # Deleting Files\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "##### Experimentatal Code ##### \n",
    "def set_retriver(collection_name):\n",
    "    langchain_chroma = Chroma(\n",
    "    client=chroma,\n",
    "    collection_name= collection_name,\n",
    "    embedding_function=embedding_function)\n",
    "    \n",
    "    retriever = langchain_chroma.as_retriever(search_kwargs={\"k\": 2, \"search_type\" : \"similarity\"})\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# Prompt Template for Langchain\n",
    "template = \"\"\"You are a helpful AI assistant. Use only the below provided Context to answer the following question. If you do not know the answer respond with \"I don't know.\"\n",
    "Context:{context}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "  \n",
    "\n",
    "def chain(query, retriever):\n",
    "    \"\"\"\n",
    "    Executes a retrieval-based question-answering chain with specified query and retriever.\n",
    "\n",
    "    Args:\n",
    "    - query (str): The query/question to be answered.\n",
    "    - retriever (Retriever): The retriever object responsible for fetching relevant documents.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the RetrievalQA.\n",
    "    \"\"\"\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       chain_type=\"stuff\", \n",
    "                                       retriever=set_retriver(retriever), \n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                                       verbose=True, tags=[\"llama-2-7b\", \"8-bit\"]\n",
    "                                       )\n",
    "    return qa_chain(query)\n",
    "\n",
    "def add_text(history, text):\n",
    "    \"\"\"\n",
    "    Adds the user's text input to the conversation history.\n",
    "\n",
    "    Args:\n",
    "    - history (list): The existing conversation history.\n",
    "    - text (str): The user's input text.\n",
    "\n",
    "    Returns:\n",
    "    - list: Updated history with the user's input.\n",
    "    - str: Empty string (reserved for future use).\n",
    "    \"\"\"\n",
    "    history = history + [(text, None)]\n",
    "    return history, \"\"\n",
    "\n",
    "def bot(history, collection):\n",
    "    \"\"\"\n",
    "    Generates a response using a Language Model and updates the conversation history.\n",
    "\n",
    "    Args:\n",
    "    - history (list): The existing conversation history.\n",
    "    - collection (str): The name of the collection used for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "    - list: Updated conversation history including the bot's response.\n",
    "    \"\"\"\n",
    "    response = llm_ans(history[-1][0], collection)\n",
    "    history[-1][1] = response\n",
    "    return history\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    \"\"\"\n",
    "    Wraps the text while preserving newlines to fit within a specified width.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    - width (int): The maximum width of the text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Wrapped text with newlines preserved.\n",
    "    \"\"\"\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    \"\"\"\n",
    "    Processes the Language Model's response by wrapping the text and printing the source documents.\n",
    "\n",
    "    Args:\n",
    "    - llm_response (dict): The response from the Language Model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The wrapped text.\n",
    "    \"\"\"\n",
    "    result = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])\n",
    "    return result    \n",
    "\n",
    " \n",
    "\n",
    "def llm_ans(query, collection):\n",
    "    \"\"\"\n",
    "    Gets the answer from the Language Model including relevant source files.\n",
    "\n",
    "    Args:\n",
    "    - query (str): The query/question to be answered.\n",
    "    - collection (str): The name of the collection used for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "    - str: The answer along with relevant source files.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    llm_response = chain(query, collection)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "\n",
    "    \n",
    "    # print(llm_response['result'])\n",
    "    sources = []\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        source_file = source.metadata['source']\n",
    "        source_file = source_file.replace(\"/home/cdsw/data/\", \"\")\n",
    "        sources.append(source_file)\n",
    "    source_files = \"\\n\".join(sources) \n",
    "    ans = llm_response['result'] + \"\\n \\n Relevant Sources: \\n\" + source_files + \"\\n \\n Elapsed Time: \" + str(round(elapsed_time,2)) + \" seconds\"\n",
    "    return ans\n",
    "\n",
    "def reset_state():\n",
    "    \"\"\"\n",
    "    Resets the Gradio UI\n",
    "    \"\"\"\n",
    "    return [], [], None\n",
    "\n",
    "#Gradio UI Code Block\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"FileGPT\"):\n",
    "        chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=650)\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                with gr.Column(scale=12):\n",
    "                    user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10).style(\n",
    "                        container=False)\n",
    "                with gr.Column(min_width=32, scale=1):\n",
    "                    submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            with gr.Column(scale=1):\n",
    "                collection_dropdown = gr.Dropdown(\n",
    "                    collection_list, label=\"Chroma Collections\", info=\"Choose a Collection to Query\",\n",
    "                    value = \"default\", max_choices=1\n",
    "                )\n",
    "                emptyBtn = gr.Button(\"Clear History\")\n",
    "        user_input.submit(add_text, [chatbot, user_input], [chatbot, user_input]).then(bot, [chatbot, collection_dropdown], chatbot)\n",
    "        submitBtn.click(add_text, [chatbot, user_input], [chatbot, user_input]).then(bot, [chatbot, collection_dropdown], chatbot)\n",
    "        history = gr.State([])\n",
    "        past_key_values = gr.State(None)\n",
    "        emptyBtn.click(reset_state, outputs=[chatbot, history, past_key_values], show_progress=True)\n",
    "        \n",
    "\n",
    "    with gr.Tab(\"Upload File\"):\n",
    "        with gr.Row():\n",
    "            title=\"Falcon 40B\",\n",
    "            with gr.Column(scale=4):\n",
    "                file_output = gr.File()\n",
    "                upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\".pdf\",\".csv\",\".doc\"], file_count=\"multiple\")\n",
    "                upload_button.upload(upload_file, upload_button, file_output)\n",
    "            with gr.Column(scale=1):\n",
    "                embed_dropdown = gr.Dropdown(\n",
    "                    collection_list, label=\"Chroma Collections\", info=\"Choose a Collection to Query\", \n",
    "                    value = \"default\", max_choices=1\n",
    "                )\n",
    "                embed_button = gr.Button(\"Embed Document\", variant=\"primary\")\n",
    "                txt_3 = gr.Textbox(value=\"\", label=\"Output\")\n",
    "                \n",
    "                \n",
    "    embed_button.click(embed_documents, embed_dropdown, show_progress=True, outputs=[txt_3])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "demo.queue()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True,\n",
    "                enable_queue=True,\n",
    "                show_error=True,\n",
    "                server_name='127.0.0.1',\n",
    "                server_port=int(\"8091\")) \n",
    "\n",
    "    print(\"Gradio app ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f429ba5-a677-41d4-a48b-5b91a1a39d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
